#dependency installation
  !pip install transformers datasets torch

#the collab file python code 

  from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments
  from datasets import Dataset
  import pandas as pd
  import torch
  
  # Load transformed dataset
  data = pd.read_csv("dataset_transformed.csv")  # Use the preprocessed dataset
  data = data.sample(n=min(150, len(data)), random_state=42)  # Ensure â‰¤150 examples
  
  # Load model and tokenizer
  model_name = "t5-small"
  tokenizer = T5Tokenizer.from_pretrained(model_name)
  model = T5ForConditionalGeneration.from_pretrained(model_name)
  
  # Prepare dataset
  dataset = Dataset.from_pandas(data)
  
  def tokenize_data(example):
      return {
          "input_ids": tokenizer(example["input"], padding="max_length", truncation=True, max_length=128)["input_ids"],
          "labels": tokenizer(example["text"], padding="max_length", truncation=True, max_length=256)["input_ids"]
      }
  
  # Tokenize dataset (batched for efficiency)
  tokenized_dataset = dataset.map(tokenize_data, batched=True, num_proc=1)  # num_proc=1 to avoid overhead on Colab
  
  # Training arguments (optimized for speed, W&B disabled)
  training_args = TrainingArguments(
      output_dir="./results",
      num_train_epochs=1,  # 1 epoch for speed
      per_device_train_batch_size=16,  # Fits T4 GPU
      fp16=True,  # Mixed precision for faster training
      save_steps=500,  # Save less frequently
      save_total_limit=1,  # Keep one checkpoint
      logging_steps=100,  # Log less frequently
      learning_rate=5e-5,  # Default LR
      max_steps=100,  # Cap at ~10-15 minutes
      report_to="none",  # Disable W&B logging
  )
  
  # Initialize Trainer
  trainer = Trainer(
      model=model,
      args=training_args,
      train_dataset=tokenized_dataset,
  )
  
  # Train
  trainer.train()
  
  # Save model
  model.save_pretrained("./fine_tuned_t5")
  tokenizer.save_pretrained("./fine_tuned_t5")

#save the model as './fine_tuned_t5' file and move it to the required project folder
